\documentclass{beamer}
\usepackage[utf8]{inputenc}
\usepackage{stmaryrd}
\usepackage{tikz}
\usepackage{listings}
\usepackage{graphics}

%\usetheme{Warsaw}

\usetikzlibrary{%
    arrows,
    arrows.meta,
    decorations,
    backgrounds,
    positioning,
    fit,
    petri,
    shadows,
    datavisualization.formats.functions,
    calc,
    shapes,
    shapes.multipart,
    matrix
}

\bibliographystyle{alpha}


\begin{document}

% titlepage {{{
\begin{frame}
\begin{center}
{\Large A tensorflow backend to SpiNNaker }

More precise: a keras backend to SpiNNaker

\vspace{2cm}

Jonas Fa{\ss}bender

\vspace{2cm}

Supervisors:

Alan Stokes, Kevin Stratford, Caoimh√≠n Laoide-Kemp

\end{center}
\end{frame}
% }}}

% What is SpiNNaker? {{{
\begin{frame}[fragile]
  \frametitle{What is SpiNNaker?}
  \setbeamercovered{invisible}
  \pause

  The project's website \cite{spinnaker_project} states:
  \begin{itemize}[<+->]
    \item \textcolor{black}{[SpiNNaker is]} a platform for
          high-performance massively parallel processing
          appropriate for the simulation of large-scale
          [spiking] neural networks in real-time, as a
          research tool for neuroscientists, computer
          scientists and roboticists

    \item \textcolor{black}{[SpiNNaker is]} an aid in the
          investigation of new computer architectures,
          which break the rules of conventional
          supercomputing, but which we hope will lead to
          fundamentally new and advantageous principles for
          energy-efficient massively-parallel computing
  \end{itemize}
\end{frame}
% }}}

% Why use SpiNNaker as a target for training deep NNs? {{{
\begin{frame}[fragile]
  \frametitle{Why use SpiNNaker as a target for training deep NNs?}
  \setbeamercovered{invisible}
  \pause

  \begin{itemize}[<+->]
    \item I don't like to wait days for training my NN to
          finish (and it will probably still suck
          afterwards)
    \item Amount of computation in training deep NNs
          increases exponentially (double every 3.4 months)
          \cite{openai2019}
    \item We'll run out of available computation (and
          energy) eventually
    \item Massively parallel, energy efficient and scalable
          systems are optimal for training NNs
  \end{itemize}
\end{frame}
% }}}

% What is tensorflow? {{{
\begin{frame}[fragile]
  \frametitle{What is tensorflow? And why the subtitle}
  \setbeamercovered{invisible}
  \pause

  \begin{itemize}[<+->]
    \item TensorFlow: Large-Scale Machine Learning on
          Heterogeneous Distributed Systems \cite{tf_2015}
    \item Represents computation as a graph
    \item Basically: abstraction over various hardware and
          software libraries and APIs (CPUs, GPUs, TPUs,
          Eigen, MPI, CUDA, OpenCL, \dots)
    \item But: no abstraction over any runtime (like
          SpiNNaker)
    \item "Easiest" way to add new tensorflow backend? XLA
          (\url{https://www.tensorflow.org/xla})
    \item Instead: backend for keras (\url{https://keras.io})
    \item Using high level conceptual graph (the actual
          layers of the NN) instead of low level
          computational graph of tensorflow
  \end{itemize}
\end{frame}
% }}}

% Challenges {{{
\begin{frame}[fragile]
  \frametitle{Challenges}
  \setbeamercovered{invisible}
  \pause

  \begin{itemize}[<+->]
    \item Writing scientific programs is hard
    \item Writing programs for embedded hardware is hard
    \item Both together? \dots
  \end{itemize}
\end{frame}
% }}}

% Overcoming them {{{
\begin{frame}[fragile]
  \frametitle{Overcoming them}
  \setbeamercovered{invisible}
  \pause

  \begin{itemize}[<+->]
    \item Is there a linear algebra library for an embedded
          environment like SpiNNaker? (LAPACKE, CBLAS,
          \dots?)
    \item Don't sleep
  \end{itemize}
\end{frame}
% }}}

% References {{{
\begin{frame}
  \frametitle{References}
  \bibliography{literature.bib}
\end{frame}
% }}}

\end{document}
