\documentclass{article}

\usepackage{epcc}
\usepackage{url}
\PassOptionsToPackage{hyphens}{url}
\usepackage[colorlinks=false, hidelinks]{hyperref}
\usepackage{amsmath}
\usepackage[toc,page]{appendix}
\usepackage[table]{xcolor}
\usepackage[marginparsep=30pt]{geometry}
\usepackage{stmaryrd}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{tabu}
\usepackage{longtable}
\usepackage{tabularx}
\usepackage{listings}
\usepackage{fancyref}
\usepackage{relsize}
\usepackage{float}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{diagbox}
\usepackage{multirow}
\usepackage{slashbox}
\usepackage{graphics}
\usepackage{booktabs}
\usepackage{natbib}
\usepackage{csquotes}

\usetikzlibrary{%
    arrows,
    arrows.meta,
    decorations,
    backgrounds,
    positioning,
    fit,
    petri,
    shadows,
    datavisualization.formats.functions,
    calc,
    shapes,
    shapes.multipart,
    matrix,
    plotmarks
}

\usepgfplotslibrary{fillbetween, statistics, dateplot}

\pgfplotsset{%
  compat=1.3,
  every non boxed x axis/.style={%
  enlarge x limits=false,
  x axis line style={}%-stealth},
  },
  every boxed x axis/.style={},
  every non boxed y axis/.style={%
  enlarge y limits=false,
  y axis line style={}%-stealth},
  },
  every boxed y axis/.style={},
}

\renewcommand{\labelenumii}{\theenumii}
\renewcommand{\theenumii}{\theenumi.\arabic{enumii}.}

\bibliographystyle{plainnat}
\bibpunct{(}{)}{;}{a}{,}{,}

\begin{document}

\pagenumbering{roman}

\title{Deep Learning on SpiNNaker: Report}
\author{Jonas Fassbender \\ \textit{jonas@fassbender.dev}}
\date{}

\makeEPCCtitle

\newpage

\begin{abstract}
\end{abstract}

\newpage

\tableofcontents

\newpage

\pagenumbering{arabic}

\section{Introduction} % {{{

According to the SpiNNaker project's website:
\begin{displayquote}
  SpiNNaker is a novel massively-parallel computer
  architecture, inspired by the fundamental structure and
  function of the human brain, which itself is composed of
  billions of simple computing elements, communicating
  using unreliable spikes \citep{spinn_proj}.
\end{displayquote}
SpiNNaker is targeted at three main areas of research:
(\romannumeral 1) neuroscience: understanding the human
brain, (\romannumeral 2) robotics: low power hardware and
(\romannumeral 3) computer science: new approach to
supercomputing and massive parallelism.
The dissertation project, which this paper reports on,
will concern itself with (\romannumeral 3) and one of the
research areas of computer science, which has emerged as
a driving force behind advancements in many fields and for
many tasks like speech and image recognition, drug
discovery and genomics: deep learning
\citep{lecun_et_al_2015}.

While deep learning is a promising field and deep neural
networks at the center of important advancements, like
described above, they face a major problem: the sheer
amount of computation needed for training them.
Researchers at OpenAI have estimated, that the amount of
computation needed for training state of the art deep
neural networks increases exponentially, doubling every
3.4 months \citep{openai2019}.
In order to cope with such unprecedented amounts of
computation and energy needed, the search for specialized
hardware is well underway.
Current state of the art hardware for accelerating the
training of deep neural networks are ASICs like TPUs and
general purpose GPUs \citep{tpus, mittal_et_al_2019}.

The goal of the dissertation project that is introduced in
this paper, will be to analyze if SpiNNaker could be an
energy efficient, scalable and fast alternative to the
above mentioned hardware.
Since deep neural networks are derived from the human
brain and nerve system \citep{goodfellow2016} and SpiNNaker
was designed to model the human brain, it seems rather
probable, that SpiNNaker will be a good target for
accelerating the training of deep neural networks.

This paper concerns itself with the dissertation project:
``Deep Learning on SpiNNaker,'' which will be conducted in
the period from May 2019 to August 2019 as the final work
of the author to achieve his Master of Science in
High Performance Computing with Data Science from the
University of Edinburgh.
The report is mostly a summary of the preliminary work
conducted in the months before the actual work on the
disseration will be conducted.

The finindings of the preliminary work and the changes made
to the original project scope are the focal point of this
report.
The original title of the disseration project was ``A
Tensorflow Backend to SpiNNaker,'' but the preliminary work
conducted to this point show, that the scope will be
redirected from implementing a backend for tensorflow---a
library for running fast linear algebra operations on
distributed, heterogenous systems, mainly designed for
implementing computationally demanding machine learning
algorithms like deep learning in a fast manner
\citep{tf2015}---to an approach focused on implementing
deep learning directly on SpiNNaker.
Because of SpiNNaker's specialized design, which works
rather contrary to that of tensorflow and current hardware
trends for building accelerators for deep learning,
interfacing between SpiNNaker's runtime and tensorflow was
deemed too difficult and not beneficial.
Instead, this dissertation aims at implementing deep
learning directly on SpiNNaker, providing an interface to
the well known deep learning library Keras \citep{keras}.
\citet{proj} shows the original disseration project's
scope.

This paper starts with presenting a brief outline of the
background of the dissertation, providing a discription of
the technologies important in this report: SpiNNaker, deep
learning and tensorflow in Section~\ref{sec:background}.
Also, the updated goal for the dissertation is given.
Afterwards, in Section~\ref{sec:related_work}, some
papers and other literature crucial for the further doings
for this project are presented.
The paper continues by giving a work plan in
Section~\ref{sec:work_plan}, before presenting a risk
analysis in Section~\ref{sec:risk_analysis}.
Afterwards the preliminary findings outlined above are
disscussed in more depth in Section~\ref{sec:prelim}.
At last, Section~\ref{sec:proposal} gives the final
project proposal and Section~\ref{sec:review} will contain
a review of a related dissertation project done in 2018:
``Deep Learning Performance on Different Architectures'' by
Spyro Nita \citep{nita_2018}.

% }}}

\section{Background} % {{{
\label{sec:background}

% TODO: pictures of SpiNNaker

% paragraph on spiking neural networks

% very general view of DNNs

% very general view of tensorflow

% benchmark idea

% }}}

\section{Related Work} % {{{
\label{sec:related_work}

% TODO: benchmark papers

% }}}

\section{Work Plan} % {{{
\label{sec:work_plan}

% }}}

\section{Risk Analysis} % {{{
\label{sec:risk_analysis}

% }}}

\section{Preliminary Findings} % {{{
\label{sec:prelim}

% main problem will be programming

% => minimize everything that is not programming
%    (benchmarking: use other benchmarks, ...)

% tf backend too difficult (XLA, linear algebra not what
% spinnaker was designed for)

% }}}

\section{Final Project Proposal} % {{{
\label{sec:proposal}

% describe the benchmark

% implement subset of keras api to make benchmark possible

% }}}

\section{Deep Learning Performance on Different % ... {{{
  Architectures: Review}
\label{sec:review}

This section will concern itself with a review of the
dissertation of Spyro Nita, which he did for earning his
Master of Science in High Performance Computing with Data
Science at the University of Edinburgh:
``Deep Learning Performance on Different Architectures''
 \citep{nita_2018}.
First, a summary of his dissertation will be given, before
the review of his work is presented.
The review will start by looking at how well the context
of the dissertation is explained and how well the scope
of the dissertation's problem is defined.
The last part of the review will be the consideration of
the dissertation's layout and formatting.
After the review, the importance of the dissertation for
``Deep Learning on SpiNNaker'' will be discussed and
evaluated.

% }}}

\bibliography{library.bib}

\end{document}
