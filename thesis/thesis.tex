\documentclass[]{article}

%\usepackage{epcc}
\usepackage{jmlr2e}
%\usepackage[hyphens]{url}
%\PassOptionsToPackage{hyphens}{url}
%\usepackage[colorlinks=false, hidelinks]{hyperref}
\usepackage{amsmath}
\usepackage[toc,page]{appendix}
\usepackage[table]{xcolor}
\usepackage[marginparsep=30pt]{geometry}
\usepackage{stmaryrd}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{tabu}
\usepackage{longtable}
\usepackage{tabularx}
\usepackage{listings}
\usepackage{fancyref}
\usepackage{relsize}
\usepackage{float}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{diagbox}
\usepackage{multirow}
\usepackage{slashbox}
\usepackage{graphics}
\usepackage{booktabs}
\usepackage{natbib}
\usepackage{csquotes}

\usetikzlibrary{%
    arrows,
    arrows.meta,
    decorations,
    backgrounds,
    positioning,
    fit,
    petri,
    shadows,
    datavisualization.formats.functions,
    calc,
    shapes,
    shapes.multipart,
    matrix,
    plotmarks
}

\usepgfplotslibrary{fillbetween, statistics, dateplot}

\pgfplotsset{%
  compat=1.3,
  every non boxed x axis/.style={%
  enlarge x limits=false,
  x axis line style={}%-stealth},
  },
  every boxed x axis/.style={},
  every non boxed y axis/.style={%
  enlarge y limits=false,
  y axis line style={}%-stealth},
  },
  every boxed y axis/.style={},
}

\renewcommand{\labelenumii}{\theenumii}
\renewcommand{\theenumii}{\theenumi.\arabic{enumii}.}

\bibliographystyle{plainnat}
\bibpunct{(}{)}{;}{a}{,}{,}

\newenvironment{declaration}
{\centerline{\large\bf Declaration}\vspace{0.7ex}%
  \bgroup\leftskip 20pt\rightskip 20pt\noindent\ignorespaces}%
{\par\egroup\vskip 0.25ex}


\def\title{Deep Learning on SpiNNaker}
\def\author{Jonas Fassbender \\ \textit{jonas@fassbender.dev}}
\date{}

\ShortHeadings{Jonas Fassbender}{\title}

\begin{document}

% titlepage {{{
\begin{titlepage}

\begin{flushleft}
	\vspace*{-1cm}
	\includegraphics[scale=0.15]{logos/logo_black.pdf}\\
	\vspace*{1cm}
\end{flushleft}
\begin{flushright}
	\vspace*{-3cm}
	\includegraphics[scale=0.2]{logos/crest_bw.pdf}\\
	\vspace*{1cm}
\end{flushright}


%\EPCCheaderLogos
\null

\begin{center}
\begin{Huge}
\textbf{\title}
\end{Huge}
~\\
~\\
~\\
\textit{\Large {\LARGE M}ASTER {\LARGE T}HESIS}
~\\
~\\
~\\
\begin{Large}
\begin{tabu} to \textwidth {Xr}
Jonas Fassbender
&\href{mailto:jonas@fassbender.dev}{jonas@fassbender.dev}
\end{tabu}
\end{Large}
~\\
~\\
~\\
\begin{large}
In the course of studies

\textit{{\Large H}IGH {\Large P}ERFORMANCE {\Large C}OMPUTING WITH {\Large D}ATA {\Large S}CIENCE}
~\\
~\\
~\\
For the degree of

\textit{{\Large M}ASTER OF {\Large S}CIENCE}
~\\
~\\
~\\
The University of Edinburgh
~\\
~\\
~\\
\begin{tabular}{rl}
  First supervisor: &Caoimh√≠n Laoide-Kemp \\
                    &EPCC, University of Edinburgh \\
  &\\
  Second supervisor: &Dr Kevin Stratford \\
                     &EPCC, University of Edinburgh \\
  &\\
  Third supervisor: &Dr Alan Stokes \\
                    &APT, University of Manchester \\
\end{tabular}
~\\
~\\
~\\
Edinburgh, August 2020
\end{large}
\end{center}
\end{titlepage}
% }}}

\pagenumbering{roman}

% here thanks

% declaration {{{
\hspace{0pt}
\vfill

\begin{declaration}
I declare that this dissertation was composed by myself, that the work
contained herein is my own except where explicitly stated otherwise in
the text, and that this work has not been submitted for any other
degree or professional qualification except as specified.
~\\
~\\
~\\
\begin{tabu}{Xc}
  &Jonas Fassbender \\
  &August 2020
\end{tabu}
\end{declaration}

\vfill
\hspace{0pt}
% }}}

\newpage

% abstract {{{
\hspace{0pt}
\vfill

\begin{abstract}
\end{abstract}

\vfill
\hspace{0pt}
% }}}

\newpage

\tableofcontents

\newpage

% listoffigures

% listoftables

\pagenumbering{arabic}

% fancy page (maybe above arabic -- depends on page numbers)

\section{Introduction}

Deep learning is revolutionizing the world.
It has become part of our daily lives as consumers, powering major
software products---from recommendation systems and translation tools
to web search \citep{lecun_et_al_2015}.
Major breakthroughs in fields like computer vision or natural language
processing were achieved through the use of deep learning
\citep{krizhevsky_et_al_2012, hinton_et_al_2012}.
It has emerged as a driving force behind discoveries in numerous
domains like particle physics, drug discovery, genomics and gaming
\citep{ciodaro_et_al_2012, ma_et_al_2015, leung_et_al_2014,
  silver_et_al_2016}.

Deep learning has become so ubiquitous that we are changing the
way we build modern hardware to account for its computational demands.
From the way edge devices like mobile phones or embedded systems are
built and modern CPUs to specialized hardware designed only for deep
learning models \citep{deng_2019, boitano_2020, perez_2017,
  jouppi_et_al_2017}.
Whole state-of-the-art supercomputers are built solely for deep
learning \citep{langston_2020}.
Hardware manufacturer are faced with a major challenge in meeting the
computational demands arising from inference, and more importantly,
training deep learning models.
OpenAI researchers have estimated that the computational costs of
training increases exponentially; approximately every 3.4 months the
cost doubles \citep{amodei_et_al_2019}.
With the end of Moore's Law \citep{loeffler_2018}, chip makers have to
get creative in scaling up computing, the same way machine learning
researchers are scaling up their models \citep{simonite_2016}.
Production and research into new hardware designs for deep learning
are well on the way.

Another field which has high computational demands for very specific
tasks and algorithms is neuroscience.
Neuroscience has long been linked to deep learning, which has its
origin in reasearch done by neuroscientists
\citep{mcculloch_et_al_1943}.
While in the recent past deep learning research has been more focused
on mathematical topics like statistics and probability theory,
optimization or linear algebra, researchers are again looking to
neuroscience to further improve the capabilities of deep
learning models \citep{marblestone_et_al_2016}.

But the algorithms developed by computational neuroscientists are not
the only aspect drawing attention from the deep learning community.
Computational neuroscience has long been trying to develop hardware
for the efficient modeling of the human brain and neuromorphic
computing---a computer architecture inspired by the biological
nervous system---has been around since the 1980s \citep{mead_1989}.
Today, neuromorphic computers are being developed to meet the
demands for efficient computing needed to run large-scale
spiking neural networks used for modeling brain
functions \citep{furber_2016}.
While being developed mainly for the task of modeling the human brain,
deep learning has been linked to neuromorphic computing,
especially in the context of commercial usability \citep{gomes_2017}.
Both the low energy demands of neuromorphic computers---such as IBM's
True North \citep{cassidy_et_al_2013} or The University of
Manchester's Spiking Neural Network Architecture (SpiNNaker)
\citep{furber_et_al_2006}---and their
scalability and massive-parallelism are intriguing for two very
important use cases of deep learning:
(\romannumeral 1) edge computing, for example robotics
and mobile devices, (\romannumeral 2) supercomputers and the
cloud-era \citep{gomes_2017}.

This thesis investigates the performance of SpiNNaker machines for
deep learning by training the state-of-the-art computer vision model
ResNet-50 under the closed division rules of the MLPerf benchmark
\citep{he_et_al_2015, mattson_et_al_2019}.
In order to benchmark ResNet-50 on SpiNNaker a prototypical
implementation was developed as part of this thesis.

\begin{itemize}
  \item here a paragraph about the results
\end{itemize}

Section~\ref{sec:background} presents the background of this thesis.
An introduction to deep learning is given, as well as an overview
of the benchmark presented in Section~\ref{sec:benchmark}.
The SpiNNaker architecture is also described and compared to
current deep learning hardware.
Related work can be found in Section~\ref{subsec:related_work}.
Section~\ref{sec:SpiDNN} presents the architecture of the
prototype developed for benchmarking.
In Section~\ref{sec:discussion} the results of the benchmark are
discussed, as well as the development process.
Section~\ref{sec:conclusion} contains the conclusion, while
Section~\ref{sec:next_steps} outlines the next steps for further
increasing the performance of SpiNNaker by enhancing the
prototype.


\section{Background} % {{{
\label{sec:background}

\subsection{An Introduction to Deep Learning} % {{{

\begin{enumerate}
  \item history of DL
  \item clarify that DNNs are statistical methods (glorified non-linear
        classifiers) not biological like SNNs
  \item concepts of the MLP:
    \begin{itemize}
      \item layers
      \item activations
      \item forward- and backward-pass
      \item SGD
      \item \dots
    \end{itemize}
  \item CNNs
\end{enumerate}
% history
% neural network as a statistical method, not biologically correct

% non-linear classifiers

% concepts: layers, activation, optimizer, forward- and backward-pass
%           hidden units, ... (chapter 6 goodfellow)

% CNN's (cnn, relu, pooling, ...)

% (Residual stuff)

% }}}

\subsection{Computer Vision: ImageNet and the ILSVRC} % {{{

\begin{enumerate}
  \item short section about imagenet and ilsvrc and their importance
        for computer vision
  \item ResNet50 and residual stuff
\end{enumerate}
% short section describing imagenet and ilsvrc and their importance
% for image recognition (computer vision) tasks

% ResNet50

% }}}

\subsection{Benchmarking Deep Learning Systems: The MLPerf Benchmark} % {{{

\begin{enumerate}
  \item short section about MLPerf (so short that I maybe add it to
        previous section. Could maybe be only a single paragraph.
\end{enumerate}
% also rather short

% what is the point

% some benchmarks

% if references are too small: add all the papers from the report

% }}}

\subsection{SpiNNaker as a Neuromorphic Computer Architecture} % {{{

\begin{enumerate}
  \item describe spinnaker and the spinnaker architecture
  \item compare to other DL accelerators (GPGPUs and TPUs)
\end{enumerate}
% spinnaker architecture

% compare to other state of the art machine learning hardware

% why didn't we just implement a backend (e.g. tensorflow?)
% because architecture is so different (MACs vs. massive parallelism
% with small messages)

% }}}

\subsection{Related Work} % {{{
\label{subsec:related_work}

\begin{enumerate}
  \item SNNToolbox for translating DNNs to SNNs (only inference)
  \item TrueNorth has a paper about its DL implementation
\end{enumerate}
% that neural thing that translates trained models to spiking nns

% find more about DL on SpiNNaker

% }}}

% }}}

\section{Deep Learning on SpiNNaker}
\label{sec:SpiDNN}

\begin{itemize}
  \item concepts (layers, neurons, \dots)
  \item communication structure (partitions and global partition manager)
  \item ping-pong
  \item graph structure (especially focused on edge and host--SpiNN
    communication)
  \item interpreting neurons as domain decomposition over linear algebra
    compute graph
  \item backward pass: gradients computed two times so comm fabric is
    not overly used by unique partitions
  \item How I crushed $n$d-kernels into a single blog of weights (same
    for 2D convolutions even though less interesting)
\end{itemize}

\section{Benchmark}
\label{sec:benchmark}

\section{Discussion}
\label{sec:discussion}

\begin{itemize}
  \item space used inefficiently (cores and memory) $\rightarrow$ better
    domain decomposition
\end{itemize}

\section{Conclusion}
\label{sec:conclusion}

\section{Next Steps}
\label{sec:next_steps}

\begin{itemize}
  \item multiple copies of the same network on the same machine
    $\rightarrow$ use all resources available
  \item better domain decomposition (SpiNNaker application graph or
    custom solution (application graph not helpful for neurons which
    become too big))
  \item smart algorithms vs.\ integrating with state-of-the-art libraries
    (investing time in stuff like SLIDE and the one paper by the Austrian
    guys about sparse connections explicitly mentioning SpiNNaker and
    neuromorphic chips or rather work on a trans-/compiler
    that efficiently translates linear algebra operations (like TF,
    PyTorch,\dots) onto SpiNNaker)
  \item integrate into compiler projects like Apache-TVM, XLA, Glow,
   nGraph, etc.
  \item implementing ONNX spec to make it easy for developers to use
    SpiNNaker (develop in PyTorch $\rightarrow$ run on SpiNNaker)
\end{itemize}

\bibliography{library.bib}
\addcontentsline{toc}{section}{References}

\end{document}
