\documentclass[]{article}

%\usepackage{epcc}
\usepackage{jmlr2e}
%\usepackage[hyphens]{url}
%\PassOptionsToPackage{hyphens}{url}
%\usepackage[colorlinks=false, hidelinks]{hyperref}
\usepackage{amsmath}
\usepackage[toc,page]{appendix}
\usepackage[table]{xcolor}
\usepackage[marginparsep=30pt]{geometry}
\usepackage{stmaryrd}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{tabu}
\usepackage{longtable}
\usepackage{tabularx}
\usepackage{listings}
\usepackage{fancyref}
\usepackage{relsize}
\usepackage{float}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{diagbox}
\usepackage{multirow}
\usepackage{slashbox}
\usepackage{graphics}
\usepackage{booktabs}
\usepackage{natbib}
\usepackage{csquotes}

\usetikzlibrary{%
    arrows,
    arrows.meta,
    decorations,
    backgrounds,
    positioning,
    fit,
    petri,
    shadows,
    datavisualization.formats.functions,
    calc,
    shapes,
    shapes.multipart,
    matrix,
    plotmarks
}

\usepgfplotslibrary{fillbetween, statistics, dateplot}

\pgfplotsset{%
  compat=1.3,
  every non boxed x axis/.style={%
  enlarge x limits=false,
  x axis line style={}%-stealth},
  },
  every boxed x axis/.style={},
  every non boxed y axis/.style={%
  enlarge y limits=false,
  y axis line style={}%-stealth},
  },
  every boxed y axis/.style={},
}

\renewcommand{\labelenumii}{\theenumii}
\renewcommand{\theenumii}{\theenumi.\arabic{enumii}.}

\bibliographystyle{plainnat}
\bibpunct{(}{)}{;}{a}{,}{,}

\newenvironment{declaration}
{\centerline{\large\bf Declaration}\vspace{0.7ex}%
  \bgroup\leftskip 20pt\rightskip 20pt\noindent\ignorespaces}%
{\par\egroup\vskip 0.25ex}


\def\title{Deep Learning on SpiNNaker}
\def\author{Jonas Fassbender \\ \textit{jonas@fassbender.dev}}
\date{}

\ShortHeadings{Jonas Fassbender}{\title}

\begin{document}

% titlepage {{{
\begin{titlepage}

\begin{flushleft}
	\vspace*{-1cm}
	\includegraphics[scale=0.15]{logos/logo_black.pdf}\\
	\vspace*{1cm}
\end{flushleft}
\begin{flushright}
	\vspace*{-3cm}
	\includegraphics[scale=0.2]{logos/crest_bw.pdf}\\
	\vspace*{1cm}
\end{flushright}


%\EPCCheaderLogos
\null

\begin{center}
\begin{Huge}
\textbf{\title}
\end{Huge}
~\\
~\\
~\\
\textit{\Large {\LARGE M}ASTER {\LARGE T}HESIS}
~\\
~\\
~\\
\begin{Large}
\begin{tabu} to \textwidth {Xr}
Jonas Fassbender
&\href{mailto:jonas@fassbender.dev}{jonas@fassbender.dev}
\end{tabu}
\end{Large}
~\\
~\\
~\\
\begin{large}
In the course of studies

\textit{{\Large H}IGH {\Large P}ERFORMANCE {\Large C}OMPUTING WITH {\Large D}ATA {\Large S}CIENCE}
~\\
~\\
~\\
For the degree of

\textit{{\Large M}ASTER OF {\Large S}CIENCE}
~\\
~\\
~\\
The University of Edinburgh
~\\
~\\
~\\
\begin{tabular}{rl}
  First supervisor: &Caoimh√≠n Laoide-Kemp \\
                    &EPCC, University of Edinburgh \\
  &\\
  Second supervisor: &Dr Kevin Stratford \\
                     &EPCC, University of Edinburgh \\
  &\\
  Third supervisor: &Dr Alan Stokes \\
                    &APT, University of Manchester \\
\end{tabular}
~\\
~\\
~\\
Edinburgh, August 2020
\end{large}
\end{center}
\end{titlepage}
% }}}

\pagenumbering{roman}

% here thanks

% declaration {{{
\hspace{0pt}
\vfill

\begin{declaration}
I declare that this dissertation was composed by myself, that the work
contained herein is my own except where explicitly stated otherwise in
the text, and that this work has not been submitted for any other
degree or professional qualification except as specified.
~\\
~\\
~\\
\begin{tabu}{Xc}
  &Jonas Fassbender \\
  &August 2020
\end{tabu}
\end{declaration}

\vfill
\hspace{0pt}
% }}}

\newpage

% abstract {{{
\hspace{0pt}
\vfill

\begin{abstract}
\end{abstract}

\vfill
\hspace{0pt}
% }}}

\newpage

\tableofcontents

\newpage

% listoffigures

% listoftables

\pagenumbering{arabic}

% fancy page (maybe above arabic -- depends on page numbers)

\section{Introduction}

Deep learning is revolutionizing the world.
It has become part of our daily lives as consumers, powering major
software products---from recommendation systems over translation tools
to web search \citep{lecun_et_al_2015}.
Major breakthroughs in fields like computer vision or natural language
processing were achieved through the use of deep learning
\citep{krizhevsky_et_al_2012, hinton_et_al_2012}.
It has emerged as a driving force behind discoveries in numerous
domains like particle physics, drug discovery, genomics or gaming
\citep{ciodaro_et_al_2012, ma_et_al_2015, leung_et_al_2014,
  silver_et_al_2016}.

Deep learning has become so ubiquitous that we are changing the
way we build modern hardware to account for its computational demands.
From the way edge devices like mobile phones or embedded systems are
build over modern CPUs to specialized hardware designed only for deep
learning models \citep{deng_2019, boitano_2020, perez_2017,
  jouppi_et_al_2017}.
Whole state-of-the-art supercomputers are build solely for deep
learning \citep{langston_2020}.
Hardware manifacturers are faced with a major challenge in meeting the
computational demands arising from inference and more importantly
training deep learning models.
OpenAI researchers have estimated that the computational costs of
training increases exponentially; approximately every 3.4 months the
cost doubles \citep{amodei_et_al_2019}.
With the end of Moore's Law chip makers have to get creative in
scaling up computing the same way machine learning researchers are
scaling up their models \citep{simonite_2016}.
Production and research into new hardware designs for deep learning
are well on the way.

Another field which has high computational demands for very specific
tasks and algorithms is neuroscience.
Neuroscience has long been linked to deep learning, which has its
origin in reasearch done by neuroscientists
\citep{mcculloch_et_al_1943}.
While in the recent past deep learning research has been more focused
on mathematical topics like statistcs and probability theory,
optimization or linear algebra, researchers are again looking over to
neuroscience in order to further improve the capabilities of deep
learning models \citep{marblestone_et_al_2016}.
But not only the algorithms used by neuroscientists are drawing
interest from the deep learning community.
Computational neuroscience has long been trying to develop hardware
for the efficient modeling of the human brain.
Neuromorphic computer architectures are being developed to meet the
demands for efficient computing for running large-scale spiking neural
networks used for modeling brain functions \citep{furber}.
% here some stuff linking dl to neuromorphic computers

% small section about spinnaker

% dl on spinnaker

% results

% layout

\section{Background}

\subsection{An Introduction to Deep Learning}

% history
% neural network as a statistical method, not biologically correct

% non-linear classifiers

% concepts: layers, activation, optimizer, forward- and backward-pass
%           hidden units, ... (chapter 6 goodfellow)

% CNN's (cnn, relu, pooling, ...)

% (Residual stuff)

\subsection{Computer Vision: ImageNet and the ILSVRC}

% short section describing imagenet and ilsvrc and their importance
% for image recognition (computer vision) tasks

% ResNet50

\subsection{Benchmarking Deep Learning Systems: The MLPerf Benchmark}

% also rather short

% what is the point

% some benchmarks

% if references are too small: add all the papers from the report

\subsection{SpiNNaker as a Neuromorphic Computer Architecture}

% spinnaker architecture

% compare to other state of the art machine learning hardware

% why didn't we just implement a backend (e.g. tensorflow?)
% because architecture is so different (MACs vs. massive parallelism
% with small messages)

\subsection{Related Work}

% that neural thing that translates trained models to spiking nns

% find more about DL on SpiNNaker


\section{Deep Learning on SpiNNaker}

\section{Benchmark}

\section{Discussion}

\section{Conclusion}

\section{Next Steps}

\bibliography{library.bib}
\addcontentsline{toc}{section}{References}

\end{document}
