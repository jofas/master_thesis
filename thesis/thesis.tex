\documentclass[]{article}

%\usepackage{epcc}
\usepackage{jmlr2e}
%\usepackage[hyphens]{url}
%\PassOptionsToPackage{hyphens}{url}
%\usepackage[colorlinks=false, hidelinks]{hyperref}
\usepackage{amsmath}
\usepackage[toc,page]{appendix}
\usepackage[table]{xcolor}
\usepackage[marginparsep=30pt]{geometry}
\usepackage{stmaryrd}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{tabu}
\usepackage{longtable}
\usepackage{tabularx}
\usepackage{listings}
\usepackage{fancyref}
\usepackage{relsize}
\usepackage{float}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{diagbox}
\usepackage{multirow}
\usepackage{slashbox}
\usepackage{graphics}
\usepackage{booktabs}
\usepackage{natbib}
\usepackage{csquotes}

\usetikzlibrary{%
    arrows,
    arrows.meta,
    decorations,
    backgrounds,
    positioning,
    fit,
    petri,
    shadows,
    datavisualization.formats.functions,
    calc,
    shapes,
    shapes.multipart,
    matrix,
    plotmarks
}

\usepgfplotslibrary{fillbetween, statistics, dateplot}

\pgfplotsset{%
  compat=1.3,
  every non boxed x axis/.style={%
  enlarge x limits=false,
  x axis line style={}%-stealth},
  },
  every boxed x axis/.style={},
  every non boxed y axis/.style={%
  enlarge y limits=false,
  y axis line style={}%-stealth},
  },
  every boxed y axis/.style={},
}

\renewcommand{\labelenumii}{\theenumii}
\renewcommand{\theenumii}{\theenumi.\arabic{enumii}.}

\bibliographystyle{plainnat}
\bibpunct{(}{)}{;}{a}{,}{,}

\newenvironment{declaration}
{\centerline{\large\bf Declaration}\vspace{0.7ex}%
  \bgroup\leftskip 20pt\rightskip 20pt\noindent\ignorespaces}%
{\par\egroup\vskip 0.25ex}


\def\title{Deep Learning on SpiNNaker}
\def\author{Jonas Fassbender \\ \textit{jonas@fassbender.dev}}
\date{}

\ShortHeadings{Jonas Fassbender}{\title}

\begin{document}

% titlepage {{{
\begin{titlepage}

\begin{flushleft}
	\vspace*{-1cm}
	\includegraphics[scale=0.15]{logos/logo_black.pdf}\\
	\vspace*{1cm}
\end{flushleft}
\begin{flushright}
	\vspace*{-3cm}
	\includegraphics[scale=0.2]{logos/crest_bw.pdf}\\
	\vspace*{1cm}
\end{flushright}


%\EPCCheaderLogos
\null

\begin{center}
\begin{Huge}
\textbf{\title}
\end{Huge}
~\\
~\\
~\\
\textit{\Large {\LARGE M}ASTER {\LARGE T}HESIS}
~\\
~\\
~\\
\begin{Large}
\begin{tabu} to \textwidth {Xr}
Jonas Fassbender
&\href{mailto:jonas@fassbender.dev}{jonas@fassbender.dev}
\end{tabu}
\end{Large}
~\\
~\\
~\\
\begin{large}
In the course of studies

\textit{{\Large H}IGH {\Large P}ERFORMANCE {\Large C}OMPUTING WITH {\Large D}ATA {\Large S}CIENCE}
~\\
~\\
~\\
For the degree of

\textit{{\Large M}ASTER OF {\Large S}CIENCE}
~\\
~\\
~\\
The University of Edinburgh
~\\
~\\
~\\
\begin{tabular}{rl}
  First supervisor: &Caoimh√≠n Laoide-Kemp \\
                    &EPCC, University of Edinburgh \\
  &\\
  Second supervisor: &Dr Kevin Stratford \\
                     &EPCC, University of Edinburgh \\
  &\\
  Third supervisor: &Dr Alan Stokes \\
                    &APT, University of Manchester \\
\end{tabular}
~\\
~\\
~\\
Edinburgh, August 2020
\end{large}
\end{center}
\end{titlepage}
% }}}

\pagenumbering{roman}

% here thanks

% declaration {{{
\hspace{0pt}
\vfill

\begin{declaration}
I declare that this dissertation was composed by myself, that the work
contained herein is my own except where explicitly stated otherwise in
the text, and that this work has not been submitted for any other
degree or professional qualification except as specified.
~\\
~\\
~\\
\begin{tabu}{Xc}
  &Jonas Fassbender \\
  &August 2020
\end{tabu}
\end{declaration}

\vfill
\hspace{0pt}
% }}}

\newpage

% abstract {{{
\hspace{0pt}
\vfill

\begin{abstract}
\end{abstract}

\vfill
\hspace{0pt}
% }}}

\newpage

\tableofcontents

\newpage

% listoffigures

% listoftables

\pagenumbering{arabic}

% fancy page (maybe above arabic -- depends on page numbers)

\section{Introduction}

Deep learning is revolutionizing the world.
It has become part of our daily lives as consumers, powering major
software products---from recommendation systems over translation tools
to web search \citep{lecun_et_al_2015}.
Major breakthroughs in fields like computer vision or natural language
processing were achieved through the use of deep learning
\citep{krizhevsky_et_al_2012, hinton_et_al_2012}.
It has emerged as a driving force behind discoveries in numerous
domains like particle physics, drug discovery, genomics or gaming
\citep{ciodaro_et_al_2012, ma_et_al_2015, leung_et_al_2014,
  silver_et_al_2016}.

Deep learning has become so ubiquitous that we are changing the
way we build modern hardware to account for its computational demands.
From the way edge devices like mobile phones or embedded systems are
build over modern CPUs to specialized hardware designed only for deep
learning models \citep{deng_2019, boitano_2020, perez_2017,
  jouppi_et_al_2017}.
Whole state-of-the-art supercomputers are build solely for deep
learning \citep{langston_2020}.
Hardware manifacturers are faced with a major challenge in meeting the
computational demands arising from inference and more importantly
training deep learning models.
OpenAI researchers have estimated that the computational costs of
training increases exponentially; approximately every 3.4 months the
cost doubles \citep{amodei_et_al_2019}.
With the end of Moore's Law chip makers have to get creative in
scaling up computing the same way machine learning researchers are
scaling up their models \citep{simonite_2016}.
Production and research into new hardware designs for deep learning
are well on the way.

Another field which has high computational demands for very specific
tasks and algorithms is neuroscience.
Neuroscience has long been linked to deep learning, which has its
origin in reasearch done by neuroscientists
\citep{mcculloch_et_al_1943}.
While in the recent past deep learning research has been more focused
on mathematical topics like statistcs and probability theory,
optimization or linear algebra, researchers are again looking over to
neuroscience in order to further improve the capabilities of deep
learning models \citep{marblestone_et_al_2016}.

But not only the algorithms used by neuroscientists are drawing
interest from the deep learning community.
Computational neuroscience has long been trying to develop hardware
for the efficient modeling of the human brain.
Neuromorphic computer architectures are being developed to meet the
demands for efficient computing for running large-scale spiking neural
networks used for modeling brain functions \citep{furber_2016}.
While being developed mainly for the task of modeling the human brain,
deep learning has been linked to neuromorphic computing for some time,
especially in the context of commercial usability.
Both the low energy demands of neuromorphic computer chips and their
general design concerning scalability and massive-parallelism are
intriguing for two very important use cases of deep learning:
(\romannumeral 1) edge computing, for example robotics
and mobile devices and (\romannumeral 2) supercomputers and the
cloud-era \citep{gomes_2017}.

One of this neuromorphic computer architectures is the Spiking Neural
Network Architecture (SpiNNaker).
This thesis investigates the performance of SpiNNaker machines for
deep learning by training the state-of-the-art computer vision model
ResNet-50 under the closed division rules of the MLPerf benchmark
\citep{he_et_al_2015, mattson_et_al_2019}.
In order to benchmark ResNet-50 on SpiNNaker a prototypical
implementation was developed as part of this thesis.

\begin{itemize}
  \item here a paragraph about the results
\end{itemize}

Section~\ref{sec:background} presents the background of this thesis.
An introduction to deep learning is given, as well as an overview
over the benchmark presented in Section~\ref{sec:benchmark}.
The SpiNNaker architecture is also described and compared to
current deep learning hardware.
Related work can be found in Section~\ref{subsec:related_work}.
Section~\ref{sec:SpiDNN} presents the architecture of the
prototype developed for benchmarking.
In Section~\ref{sec:discussion} the results of the benchmark are
discussed as well as the development process.
Section~\ref{sec:conclusion} contains the conclusion while
Section~\ref{sec:next_steps} outlines the next steps for further
increasing the performance of SpiNNaker by enhancing the
prototype.


\section{Background} % {{{
\label{sec:background}

\subsection{An Introduction to Deep Learning} % {{{

\begin{enumerate}
  \item history of DL
  \item clearify that DNNs are statistical methods (glorified non-linear
        classifiers) not biological like SNNs
  \item concepts of the MLP:
    \begin{itemize}
      \item layers
      \item activations
      \item forward- and backward-pass
      \item SGD
      \item \dots
    \end{itemize}
  \item CNNs
\end{enumerate}
% history
% neural network as a statistical method, not biologically correct

% non-linear classifiers

% concepts: layers, activation, optimizer, forward- and backward-pass
%           hidden units, ... (chapter 6 goodfellow)

% CNN's (cnn, relu, pooling, ...)

% (Residual stuff)

% }}}

\subsection{Computer Vision: ImageNet and the ILSVRC} % {{{

\begin{enumerate}
  \item short section about imagenet and ilsvrc and their importance
        for computer vision
  \item ResNet50 and residual stuff
\end{enumerate}
% short section describing imagenet and ilsvrc and their importance
% for image recognition (computer vision) tasks

% ResNet50

% }}}

\subsection{Benchmarking Deep Learning Systems: The MLPerf Benchmark} % {{{

\begin{enumerate}
  \item short section about MLPerf (so short that I maybe add it to
        previous section. Could maybe be only a single paragraph.
\end{enumerate}
% also rather short

% what is the point

% some benchmarks

% if references are too small: add all the papers from the report

% }}}

\subsection{SpiNNaker as a Neuromorphic Computer Architecture} % {{{

\begin{enumerate}
  \item describe spinnaker and the spinnaker architecture
  \item compare to other DL accelerators (GPGPUs and TPUs)
\end{enumerate}
% spinnaker architecture

% compare to other state of the art machine learning hardware

% why didn't we just implement a backend (e.g. tensorflow?)
% because architecture is so different (MACs vs. massive parallelism
% with small messages)

% }}}

\subsection{Related Work} % {{{
\label{subsec:related_work}

\begin{enumerate}
  \item SNNToolbox for translating DNNs to SNNs (only inference)
  \item TrueNorth has a paper about its DL implementation
\end{enumerate}
% that neural thing that translates trained models to spiking nns

% find more about DL on SpiNNaker

% }}}

\section{Deep Learning on SpiNNaker}
\label{sec:SpiDNN}

\section{Benchmark}
\label{sec:benchmark}

\section{Discussion}
\label{sec:discussion}

\section{Conclusion}
\label{sec:conclusion}

\section{Next Steps}
\label{sec:next_steps}

\bibliography{library.bib}
\addcontentsline{toc}{section}{References}

\end{document}
